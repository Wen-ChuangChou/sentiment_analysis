{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd97d42e-3d03-45d2-9bfd-76752c6712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datetime import datetime\n",
    "# import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15cdfb5a-a159-4548-909f-522fa467a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "model_path = os.path.join(\"./pretrained_llms\", model_name)\n",
    "data_path = \"./data\"\n",
    "data_name = \"mteb/tweet_sentiment_extraction\"\n",
    "cache_dir = \"./cache\"\n",
    "output_dir=\"./results\"\n",
    "\n",
    "dataset = load_dataset(data_name, cache_dir=data_path)\n",
    "# dataset = load_dataset(data_name, cache_dir=data_path, split='train[10:20]')\n",
    "# dataset = load_dataset(data_name, cache_dir=data_path, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22d9b0-74db-4acb-bf9f-119042fbe5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          add_eos_token=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"No pad token found in tokenizer, setting pad token to eos token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "if model.config.pad_token_id is None:\n",
    "    print(\"No pad token found in model, setting pad token to eos token of tokenizer\")\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.padding_side = \"right\"\n",
    "    model.config.use_cache = False  # This can help with training stability\n",
    "    model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b030998-926d-4de9-88a6-d62dfb92f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)   \n",
    "# apply tokenizer function on your data\n",
    "tokenized_data = dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "# train = tokenized_data['train'].select(range(10000))\n",
    "train = tokenized_data['train']\n",
    "test = tokenized_data['test']\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d87870-e919-4841-867d-a3fc3eecd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the tokenizer settings:\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"Model pad token ID: {model.config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432df975-6623-44d2-81dd-7bfe3fbd2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the accuracy metric\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55ff2d-912d-4234-8a41-87e352532da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Compute individual metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f4d45-3056-4b08-bcc9-66ec5623b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    learning_rate=1e-5,  # Experiment with different rates\n",
    "    # lr_scheduler_type=\"linear\",  # Add learning rate scheduling\n",
    "    # warmup_steps=100,  # Implement learning rate warmup\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,   \n",
    "    eval_strategy='steps',\n",
    "    logging_steps=250,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    # eval_steps=50,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,   \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f34de-d643-4434-8887-eb14e4d17e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db60b8-2883-41e6-9d88-85106267159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = os.path.join(\"./saved_model\", model_name)\n",
    "trainer.save_model(save_model_path)\n",
    "tokenizer.save_pretrained(save_model_path)\n",
    "print(f\"Fine-tuned model saved to: {save_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac738953-df54-46b5-bf34-f0e1e986426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(eval_dataset=train) #evaluate train dataset\n",
    "eval_metrics = trainer.evaluate(eval_dataset=test) #evaluate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5381988-ef01-41f7-9f86-26e47212b041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7634c0331904524ade3cb27ec91e9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load saved model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./saved_model/Llama-3.1-8B-Instruct\",\n",
    "    num_labels=3,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./saved_model/Llama-3.1-8B-Instruct\", add_eos_token=True, cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c973caaa-9767-4025-b028-badcf6fdf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(dataset, results, verbose=False):\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(results)\n",
    "    confusion_matrix = {\n",
    "        'negative': {'negative': 0, 'neutral': 0, 'positive': 0},\n",
    "        'neutral': {'negative': 0, 'neutral': 0, 'positive': 0},\n",
    "        'positive': {'negative': 0, 'neutral': 0, 'positive': 0}\n",
    "    }\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        true_label = dataset[i]['label_text']\n",
    "        # predicted = result['predicted_sentiment'].lower().strip()\n",
    "        predicted = result['predicted_sentiment']        \n",
    "        \n",
    "        # Handle variations in predictions\n",
    "        if 'positive' in predicted:\n",
    "            predicted = 'positive'\n",
    "        elif 'negative' in predicted:\n",
    "            predicted = 'negative'\n",
    "        elif 'neutral' in predicted:\n",
    "            predicted = 'neutral'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"Warning: Unexpected prediction format in {i}th data: {predicted}\")\n",
    "            continue\n",
    "            \n",
    "        is_correct = true_label == predicted\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        # Update confusion matrix\n",
    "        confusion_matrix[true_label][predicted] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"True label: {true_label}\")\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            print(f\"Correct: {is_correct}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Correct predictions: {correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    print(f\"{'True|Pred':<15}{'Negative':<10}{'Neutral':<10}{'Positive':<10}\")\n",
    "    for true_label in ['negative', 'neutral', 'positive']:\n",
    "        row = confusion_matrix[true_label]\n",
    "        print(f\"{true_label.title():<15}{row['negative']:<10}{row['neutral']:<10}{row['positive']:<10}\")        \n",
    "        \n",
    "    # Calculate per-class metrics\n",
    "    print(\"\\n=== Per-Class Metrics ===\")\n",
    "    for label in ['negative', 'neutral', 'positive']:\n",
    "        true_pos = confusion_matrix[label][label]\n",
    "        false_pos = sum(conf[label] for l, conf in confusion_matrix.items() if l != label)\n",
    "        false_neg = sum(confusion_matrix[label].values()) - true_pos\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{label.title()} class:\")\n",
    "        print(f\"Precision: {precision:.2%}\")\n",
    "        print(f\"Recall: {recall:.2%}\")\n",
    "        print(f\"F1-score: {f1:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'total_samples': total,\n",
    "        'correct_predictions': correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656889a7-9f3c-4fb1-bcf3-9e6867154b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(results, model_name, data_name, base_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Save analysis results to CSV file with automatic filename generation and collision handling.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries containing analysis results\n",
    "        model_name: Name of the model used\n",
    "        data_name: Name of the dataset used\n",
    "        base_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create base filename\n",
    "    dataset_prefix = data_name.split('/')[-1][:5]  # Take first 5 letters\n",
    "    base_filename = f\"finetune_{model_name}_{dataset_prefix}\"\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Generate filename with collision handling\n",
    "    filename = f\"{base_filename}.csv\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{base_filename}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def load_analysis_results(filepath):\n",
    "    \"\"\"\n",
    "    Load analysis results from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing analysis results\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Results file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    results = df.to_dict('records')\n",
    "    print(f\"Loaded {len(results)} results from: {filepath}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function to clean up checkpoints\n",
    "def cleanup_checkpoints(cache_dir=\"./cache\"):\n",
    "    \"\"\"Remove all checkpoint files from the cache directory.\"\"\"\n",
    "    if os.path.exists(cache_dir):\n",
    "        for file in os.listdir(cache_dir):\n",
    "            if file.startswith(\"checkpoint_\") and file.endswith(\".json\"):\n",
    "                os.remove(os.path.join(cache_dir, file))\n",
    "        print(\"Checkpoints cleaned up\")\n",
    "\n",
    "\n",
    "def save_metrics(metrics, model_name, data_name, base_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Save metrics to JSON file with automatic filename generation and collision handling.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        model_name: Name of the model used\n",
    "        data_name: Name of the dataset used\n",
    "        base_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create base filename\n",
    "    dataset_prefix = data_name.split('/')[-1][:5]\n",
    "    base_filename = f\"finetune_{model_name}_{dataset_prefix}_metrics\"\n",
    "    \n",
    "    # Generate filename with collision handling\n",
    "    filename = f\"{base_filename}.json\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{base_filename}_{timestamp}.json\"\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def load_metrics(filepath):\n",
    "    \"\"\"\n",
    "    Load metrics from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Metrics file not found: {filepath}\")\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"Loaded metrics from: {filepath}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8b1bd7-ef19-4ceb-8e24-8628996f1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)   \n",
    "# apply tokenizer function on your data\n",
    "tokenized_data = dataset.map(tokenizer_function, batched=True)\n",
    "\n",
    "# train = tokenized_data['train'].select(range(10000))\n",
    "train = tokenized_data['train']\n",
    "test = tokenized_data['test']\n",
    "\n",
    "# Create pipeline\n",
    "classifier = pipeline(\n",
    "    task=\"sentiment-analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device_map=\"auto\", \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "results = []\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "for text, out in zip(test['text'], classifier(KeyDataset(test, \"text\"), batch_size=16)):\n",
    "    label_id = int(out['label'].split('_')[-1])\n",
    "    sentiment = label_map[label_id]\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'predicted_sentiment': sentiment\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2ee7d9c-6083-422a-9ae2-34c435264de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "0\n",
      "negative\n",
      "I THINK EVERYONE HATES ME ON HERE   lol\n"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "print(results[id]['predicted_sentiment'])\n",
    "print(test['label'][id])\n",
    "print(test['label_text'][id])\n",
    "print(test['text'][id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca244e8c-2d7a-4981-b348-5d832fa304a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./results/finetune_Llama-3.1-8B-Instruct_tweet.csv\n"
     ]
    }
   ],
   "source": [
    "results_filepath = save_analysis_results(results, model_name, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "730cb5c4-4693-45a2-b34e-66f830f2fc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "Total samples: 3534\n",
      "Correct predictions: 2880\n",
      "Accuracy: 81.49%\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "True|Pred      Negative  Neutral   Positive  \n",
      "Negative       845       145       11        \n",
      "Neutral        199       1096      135       \n",
      "Positive       25        139       939       \n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "\n",
      "Negative class:\n",
      "Precision: 79.05%\n",
      "Recall: 84.42%\n",
      "F1-score: 81.64%\n",
      "\n",
      "Neutral class:\n",
      "Precision: 79.42%\n",
      "Recall: 76.64%\n",
      "F1-score: 78.01%\n",
      "\n",
      "Positive class:\n",
      "Precision: 86.54%\n",
      "Recall: 85.13%\n",
      "F1-score: 85.83%\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_predictions(test, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bdcf4ed-d246-44a8-b370-f1ab89e52b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565137cf7ebc4b0a93b1e1a7f7970f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3887167-6d1b-4bac-9a53-9a51cbdf0fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1316316bed41cda18a0f3e59a090f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765289da7b144d9eaca03f4f3a778b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f423cb28fe334c8288d6621ac9a44daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ca15252d4e47b394bf75077457ccc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2264349a9048778b000a063c1a31b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B/commit/87c60fa09ffb46d4fbf2c315e15ec76b6bc7f9c3', commit_message='Upload LlamaForSequenceClassification', commit_description='', oid='87c60fa09ffb46d4fbf2c315e15ec76b6bc7f9c3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B', endpoint='https://huggingface.co', repo_type='model', repo_id='Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Finetune4Sentiment-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48a4fc39-c689-444d-af04-4e47a5d38a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7eb5e99efd46fd842c265ae826f399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75144e2d81d54931afafd264ba8c07b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B/commit/4534d9bbfc2bd781b7f11e2c17eec369263da5d3', commit_message='Upload tokenizer', commit_description='', oid='4534d9bbfc2bd781b7f11e2c17eec369263da5d3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B', endpoint='https://huggingface.co', repo_type='model', repo_id='Thomas-Chou/Finetune4Sentiment-Llama-3.1-8B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"Finetune4Sentiment-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b15c1-b66a-406d-9350-e2789b69bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test)\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed348ea-0782-4b8c-a87e-2f8603a10123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config.num_labels)  # Confirm matches actual label count\n",
    "print(len(np.unique(train['label'])))  # Check actual unique label count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fe7e2-ffa6-4173-b034-8732878e3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0].keys())  # Verify label column exists\n",
    "print(train[1]['input_ids'])  # Confirm label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0462c-d13c-4212-a63b-aa3e43751aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = classifier.tokenizer([\"Example text\", \"I am a boy\"], padding=True, truncation=True)\n",
    "print(tokens)\n",
    "print(classifier.tokenizer.pad_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou1_kernel",
   "language": "python",
   "name": "chou1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
