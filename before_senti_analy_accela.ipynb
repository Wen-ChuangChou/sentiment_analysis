{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd97d42e-3d03-45d2-9bfd-76752c6712a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#To do: use accelerator to distribute model in multiple GPUs\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "accelerator = Accelerator()\n",
    "set_seed(42)\n",
    "device = accelerator.device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efe6ef3-b216-43ad-835a-b02f5efaef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./pretrained_llms/Llama-3.3-70B-Instruct\"\n",
    "# model_name = \"Llama-3.1-8B-Instruct\"\n",
    "model_name = \"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "model_path = os.path.join(\"./pretrained_llms\", model_name)\n",
    "data_path = \"./data\"\n",
    "data_name = \"mteb/tweet_sentiment_extraction\"\n",
    "cache_dir = \"./cache\"\n",
    "output_dir=\"./results\"\n",
    "\n",
    "dataset = load_dataset(data_name, cache_dir=data_path, split='train'[:200])\n",
    "# dataset = load_dataset(data_name, cache_dir=data_path, split='train')\n",
    "# dataset = load_dataset(data_name, cache_dir=data_path, split='train', remove_columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03240dc3-eada-49ba-8561-fb34b3d579f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_zero_shot(dataset, model, tokenizer, device, verbose=False, num_samples=None, \n",
    "                              checkpoint_interval=100, model_name=\"model\", data_name=\"data\", \n",
    "                              cache_dir=\"./cache\"):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using zero-shot learning with checkpoint saving and resumption capabilities.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing texts to analyze\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        device: Device to run the model on\n",
    "        verbose: If True, prints each text and its prediction\n",
    "        num_samples: Optional number of samples to process (None for all)\n",
    "        checkpoint_interval: Number of items to process before saving checkpoint\n",
    "        model_name: Name of the model for checkpoint filename\n",
    "        data_name: Name of the dataset for checkpoint filename\n",
    "        cache_dir: Directory to save checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing text and predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # model = model.to(device)\n",
    "    \n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate checkpoint filename\n",
    "    dataset_prefix = data_name.split('/')[-1][:5]\n",
    "    checkpoint_filename = f\"checkpoint_{model_name}_{dataset_prefix}.json\"\n",
    "    checkpoint_path = os.path.join(cache_dir, checkpoint_filename)\n",
    "    \n",
    "    # Initialize or load checkpoint\n",
    "    start_idx = 0\n",
    "    results = []\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, 'r') as f:\n",
    "                checkpoint_data = json.load(f)\n",
    "                results = checkpoint_data['results']\n",
    "                start_idx = checkpoint_data['next_idx']\n",
    "                print(f\"Resuming from checkpoint at index {start_idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            print(\"Starting from beginning...\")\n",
    "    \n",
    "    prompt_template = \"\"\"[INST] Analyze the sentiment of the following text. Respond with exactly one word: either 'positive', 'negative', or 'neutral'.\n",
    "\n",
    "Text: \"{}\"\n",
    "\n",
    "Sentiment: [/INST]\"\"\"\n",
    "    \n",
    "    # Handle num_samples\n",
    "    texts = dataset['text']\n",
    "    if num_samples is not None:\n",
    "        texts = texts[:num_samples]\n",
    "    \n",
    "    total = len(texts)\n",
    "    \n",
    "    def save_checkpoint(current_idx, current_results):\n",
    "        checkpoint_data = {\n",
    "            'next_idx': current_idx + 1,\n",
    "            'results': current_results,\n",
    "            'total': total,\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        with open(checkpoint_path, 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=4)\n",
    "        if verbose:\n",
    "            print(f\"\\nCheckpoint saved at index {current_idx}\")\n",
    "    \n",
    "    try:\n",
    "        for i in range(start_idx, total):\n",
    "            text = texts[i]\n",
    "            prompt = prompt_template.format(text)\n",
    "            # inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=20,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.1,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "                \n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            sentiment = response[len(prompt):].strip().split()[0].lower()\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'predicted_sentiment': sentiment\n",
    "            })\n",
    "            \n",
    "            # Verbose output\n",
    "            if verbose:\n",
    "                print(f\"\\nText [{i+1}/{total}]: {text}\")\n",
    "                print(f\"Predicted sentiment: {sentiment}\")\n",
    "                \n",
    "            # Print progress every 10% if not verbose\n",
    "            elif (i + 1) % max(1, total // 10) == 0:\n",
    "                print(f\"Progress: {(i + 1) / total:.1%}\")\n",
    "            \n",
    "            # Save checkpoint at intervals\n",
    "            if (i + 1) % checkpoint_interval == 0:\n",
    "                save_checkpoint(i, results)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user. Saving checkpoint...\")\n",
    "        save_checkpoint(i, results)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {e}\")\n",
    "        print(\"Saving checkpoint...\")\n",
    "        save_checkpoint(i, results)\n",
    "        raise\n",
    "    \n",
    "    # Process completed successfully, remove checkpoint file\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"Processing completed, checkpoint file removed\")\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18b279-3a98-4bbf-a45e-2b650ea78284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baseline(dataset):\n",
    "    # Load a pre-trained sentiment model\n",
    "    baseline_classifier = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    texts = dataset['text']\n",
    "    \n",
    "    for text in texts:\n",
    "        result = baseline_classifier(text)[0]\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"Baseline model prediction: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff3e58b-a236-4ac0-8ea3-e5388adbeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(dataset, results, verbose=False):\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(results)\n",
    "    confusion_matrix = {\n",
    "        'negative': {'negative': 0, 'neutral': 0, 'positive': 0},\n",
    "        'neutral': {'negative': 0, 'neutral': 0, 'positive': 0},\n",
    "        'positive': {'negative': 0, 'neutral': 0, 'positive': 0}\n",
    "    }\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        true_label = dataset[i]['label_text']\n",
    "        # predicted = result['predicted_sentiment'].lower().strip()\n",
    "        predicted = result['predicted_sentiment']        \n",
    "        \n",
    "        # Handle variations in predictions\n",
    "        if 'positive' in predicted:\n",
    "            predicted = 'positive'\n",
    "        elif 'negative' in predicted:\n",
    "            predicted = 'negative'\n",
    "        elif 'neutral' in predicted:\n",
    "            predicted = 'neutral'\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"Warning: Unexpected prediction format in {i}th data: {predicted}\")\n",
    "            continue\n",
    "            \n",
    "        is_correct = true_label == predicted\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        # Update confusion matrix\n",
    "        confusion_matrix[true_label][predicted] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nText: {result['text']}\")\n",
    "            print(f\"True label: {true_label}\")\n",
    "            print(f\"Predicted: {predicted}\")\n",
    "            print(f\"Correct: {is_correct}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Correct predictions: {correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    print(f\"{'True|Pred':<15}{'Negative':<10}{'Neutral':<10}{'Positive':<10}\")\n",
    "    for true_label in ['negative', 'neutral', 'positive']:\n",
    "        row = confusion_matrix[true_label]\n",
    "        print(f\"{true_label.title():<15}{row['negative']:<10}{row['neutral']:<10}{row['positive']:<10}\")        \n",
    "        \n",
    "    # Calculate per-class metrics\n",
    "    print(\"\\n=== Per-Class Metrics ===\")\n",
    "    for label in ['negative', 'neutral', 'positive']:\n",
    "        true_pos = confusion_matrix[label][label]\n",
    "        false_pos = sum(conf[label] for l, conf in confusion_matrix.items() if l != label)\n",
    "        false_neg = sum(confusion_matrix[label].values()) - true_pos\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{label.title()} class:\")\n",
    "        print(f\"Precision: {precision:.2%}\")\n",
    "        print(f\"Recall: {recall:.2%}\")\n",
    "        print(f\"F1-score: {f1:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'total_samples': total,\n",
    "        'correct_predictions': correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11603b64-2779-4c2a-8f0e-2ec4d3a1ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inconsistencies(dataset, results):\n",
    "    inconsistent_data = []\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        true_label = dataset[i]['label_text']\n",
    "        predicted = result['predicted_sentiment'].lower().strip()\n",
    "        \n",
    "        # Handle variations in predictions\n",
    "        if 'positive' in predicted:\n",
    "            predicted = 'positive'\n",
    "        elif 'negative' in predicted:\n",
    "            predicted = 'negative'\n",
    "        elif 'neutral' in predicted:\n",
    "            predicted = 'neutral'\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected prediction format: {predicted}\")\n",
    "            continue\n",
    "            \n",
    "        # If prediction doesn't match ground truth, add to list\n",
    "        if true_label != predicted:\n",
    "            inconsistent_data.append({\n",
    "                'Ground Truth': true_label,\n",
    "                'Predicted': predicted,\n",
    "                'Text': dataset[i]['text']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_inconsistencies = pd.DataFrame(inconsistent_data)\n",
    "    \n",
    "    # If there are inconsistencies, display summary\n",
    "    if len(df_inconsistencies) > 0:\n",
    "        print(f\"\\nFound {len(df_inconsistencies)} inconsistencies out of {len(results)} samples\")\n",
    "        print(f\"Inconsistency rate: {(len(df_inconsistencies)/len(results)):.2%}\")\n",
    "    else:\n",
    "        print(\"\\nNo inconsistencies found!\")\n",
    "    \n",
    "    return df_inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4016be-9b8b-43bf-b996-e87de4349635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inconsistencies(dataset, results):\n",
    "    df = find_inconsistencies(dataset, results)\n",
    "    \n",
    "    # Add text length column\n",
    "    df['Text Length'] = df['Text'].str.len()\n",
    "    \n",
    "    # Add mismatch type column\n",
    "    df['Mismatch Type'] = df.apply(\n",
    "        lambda x: 'Major' if (x['Ground Truth'] == 'positive' and x['Predicted'] == 'negative') or \n",
    "                            (x['Ground Truth'] == 'negative' and x['Predicted'] == 'positive') \n",
    "        else 'Minor', axis=1\n",
    "    )\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nMismatch Type Distribution:\")\n",
    "    print(df['Mismatch Type'].value_counts())\n",
    "    \n",
    "    print(\"\\nAverage Text Length by Mismatch Type:\")\n",
    "    print(df.groupby('Mismatch Type')['Text Length'].mean())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf72da-35a2-4b03-b515-c153cb0726cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inconsistencies_with_styling(dataset, results):\n",
    "    df = find_inconsistencies(dataset, results)\n",
    "    \n",
    "    # Add styling to make it more readable\n",
    "    styled_df = df.style.set_properties(**{\n",
    "        'background-color': 'lightgray',\n",
    "        'border-color': 'black',\n",
    "        'border-style': 'solid',\n",
    "        'border-width': '1px',\n",
    "        'text-align': 'left'\n",
    "    })\n",
    "    \n",
    "    # Add color coding for different types of mismatches\n",
    "    def highlight_mismatches(row):\n",
    "        if row['Ground Truth'] == 'positive' and row['Predicted'] == 'negative':\n",
    "            return ['background-color: #ffcdd2'] * 3  # Red for major mismatches\n",
    "        elif row['Ground Truth'] == 'negative' and row['Predicted'] == 'positive':\n",
    "            return ['background-color: #ffcdd2'] * 3\n",
    "        else:\n",
    "            return ['background-color: #fff9c4'] * 3  # Yellow for minor mismatches\n",
    "    \n",
    "    styled_df = styled_df.apply(highlight_mismatches, axis=1)\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef179d2e-ee49-4a56-b684-25c29b473553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(results, model_name, data_name, base_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Save analysis results to CSV file with automatic filename generation and collision handling.\n",
    "    \n",
    "    Args:\n",
    "        results: List of dictionaries containing analysis results\n",
    "        model_name: Name of the model used\n",
    "        data_name: Name of the dataset used\n",
    "        base_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create base filename\n",
    "    dataset_prefix = data_name.split('/')[-1][:5]  # Take first 5 letters\n",
    "    base_filename = f\"before_{model_name}_{dataset_prefix}\"\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Generate filename with collision handling\n",
    "    filename = f\"{base_filename}.csv\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{base_filename}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def load_analysis_results(filepath):\n",
    "    \"\"\"\n",
    "    Load analysis results from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing analysis results\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Results file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    results = df.to_dict('records')\n",
    "    print(f\"Loaded {len(results)} results from: {filepath}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function to clean up checkpoints\n",
    "def cleanup_checkpoints(cache_dir=\"./cache\"):\n",
    "    \"\"\"Remove all checkpoint files from the cache directory.\"\"\"\n",
    "    if os.path.exists(cache_dir):\n",
    "        for file in os.listdir(cache_dir):\n",
    "            if file.startswith(\"checkpoint_\") and file.endswith(\".json\"):\n",
    "                os.remove(os.path.join(cache_dir, file))\n",
    "        print(\"Checkpoints cleaned up\")\n",
    "\n",
    "\n",
    "def save_metrics(metrics, model_name, data_name, base_dir=\"./results\"):\n",
    "    \"\"\"\n",
    "    Save metrics to JSON file with automatic filename generation and collision handling.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        model_name: Name of the model used\n",
    "        data_name: Name of the dataset used\n",
    "        base_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the saved file\n",
    "    \"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Create base filename\n",
    "    dataset_prefix = data_name.split('/')[-1][:5]\n",
    "    base_filename = f\"before_{model_name}_{dataset_prefix}_metrics\"\n",
    "    \n",
    "    # Generate filename with collision handling\n",
    "    filename = f\"{base_filename}.json\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{base_filename}_{timestamp}.json\"\n",
    "        filepath = os.path.join(base_dir, filename)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    print(f\"Metrics saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def load_metrics(filepath):\n",
    "    \"\"\"\n",
    "    Load metrics from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Metrics file not found: {filepath}\")\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"Loaded metrics from: {filepath}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c9311-38de-4d60-adb3-44bda29f0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895d7af23ea24c1fae5edcb33827beb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=\"auto\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    add_eos_token=True,    \n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"No pad token found, setting pad token to eos token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20999d0-5507-4249-90c7-79255f5ea2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# print(\"=== Testing Raw Outputs ===\")\n",
    "# test_raw_outputs(dataset, model, tokenizer, device)\n",
    "\n",
    "print(\"\\n=== Testing with Instruction Prompts ===\")\n",
    "# results = analyze_sentiment_zero_shot(dataset, model, tokenizer, device)\n",
    "# results = analyze_sentiment_zero_shot(dataset, model, tokenizer, device, verbose=False, num_samples=20)\n",
    "try:\n",
    "    results = analyze_sentiment_zero_shot(\n",
    "        dataset=dataset,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        verbose=False,\n",
    "        # num_samples=20,\n",
    "        checkpoint_interval=100,\n",
    "        model_name=model_name,\n",
    "        data_name=data_name\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProcessing interrupted. You can resume later using the same function call.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    print(\"You can resume processing later using the same function call.\")\n",
    "\n",
    "\n",
    "# Save results\n",
    "results_filepath = save_analysis_results(results, model_name, data_name)\n",
    "\n",
    "# print(\"\\n=== Comparing with Baseline Model ===\")\n",
    "# compare_with_baseline(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be2e809-2b87-419c-ba55-39525d83c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3534 results from: ./results/before_test_Llama-3.1-8B-Instruct_tweet.csv\n"
     ]
    }
   ],
   "source": [
    "results = load_analysis_results(\"./results/before_test_Llama-3.1-8B-Instruct_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ece5241-a8b4-4e1a-8cbf-8e59f7ce445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected prediction format in 17th data: gative\n",
      "Warning: Unexpected prediction format in 61th data: utral\n",
      "Warning: Unexpected prediction format in 112th data: eutral\n",
      "Warning: Unexpected prediction format in 161th data: ositive\n",
      "Warning: Unexpected prediction format in 167th data: utral\n",
      "Warning: Unexpected prediction format in 196th data: eutral\n",
      "Warning: Unexpected prediction format in 253th data: ositive\n",
      "Warning: Unexpected prediction format in 330th data: reasoning\n",
      "Warning: Unexpected prediction format in 452th data: ositive\n",
      "Warning: Unexpected prediction format in 482th data: egative\n",
      "Warning: Unexpected prediction format in 670th data: ositive\n",
      "Warning: Unexpected prediction format in 687th data: ositive\n",
      "Warning: Unexpected prediction format in 730th data: gative\n",
      "Warning: Unexpected prediction format in 812th data: eutral\n",
      "Warning: Unexpected prediction format in 831th data: ositive\n",
      "Warning: Unexpected prediction format in 876th data: egative\n",
      "Warning: Unexpected prediction format in 940th data: utral\n",
      "Warning: Unexpected prediction format in 1071th data: sitive\n",
      "Warning: Unexpected prediction format in 1106th data: egative\n",
      "Warning: Unexpected prediction format in 1143th data: explanation:\n",
      "Warning: Unexpected prediction format in 1198th data: ositive\n",
      "Warning: Unexpected prediction format in 1208th data: ositive\n",
      "Warning: Unexpected prediction format in 1296th data: eutral\n",
      "Warning: Unexpected prediction format in 1459th data: egative\n",
      "Warning: Unexpected prediction format in 1465th data: ositive\n",
      "Warning: Unexpected prediction format in 1664th data: ositive\n",
      "Warning: Unexpected prediction format in 1877th data: egative\n",
      "Warning: Unexpected prediction format in 1940th data: tral\n",
      "Warning: Unexpected prediction format in 1958th data: gative\n",
      "Warning: Unexpected prediction format in 1977th data: ositive\n",
      "Warning: Unexpected prediction format in 2124th data: itive\n",
      "Warning: Unexpected prediction format in 2196th data: egative\n",
      "Warning: Unexpected prediction format in 2214th data: gative\n",
      "Warning: Unexpected prediction format in 2446th data: itive\n",
      "Warning: Unexpected prediction format in 2500th data: ositive\n",
      "Warning: Unexpected prediction format in 2604th data: egative\n",
      "Warning: Unexpected prediction format in 2761th data: eutral\n",
      "Warning: Unexpected prediction format in 2892th data: ositive\n",
      "Warning: Unexpected prediction format in 2894th data: ositive\n",
      "Warning: Unexpected prediction format in 2968th data: egative\n",
      "Warning: Unexpected prediction format in 3121th data: egative\n",
      "Warning: Unexpected prediction format in 3233th data: ositive\n",
      "Warning: Unexpected prediction format in 3319th data: eutral\n",
      "Warning: Unexpected prediction format in 3440th data: egative\n",
      "Warning: Unexpected prediction format in 3447th data: egative\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Total samples: 3534\n",
      "Correct predictions: 2241\n",
      "Accuracy: 63.41%\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "True|Pred      Negative  Neutral   Positive  \n",
      "Negative       753       170       65        \n",
      "Neutral        358       694       358       \n",
      "Positive       63        234       794       \n",
      "\n",
      "=== Per-Class Metrics ===\n",
      "\n",
      "Negative class:\n",
      "Precision: 64.14%\n",
      "Recall: 76.21%\n",
      "F1-score: 69.66%\n",
      "\n",
      "Neutral class:\n",
      "Precision: 63.21%\n",
      "Recall: 49.22%\n",
      "F1-score: 55.34%\n",
      "\n",
      "Positive class:\n",
      "Precision: 65.24%\n",
      "Recall: 72.78%\n",
      "F1-score: 68.80%\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_predictions(dataset, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da2831-ff09-47db-af50-19734a2ac231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_inconsistencies(dataset, results)\n",
    "# Display full DataFrame (not truncated)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nInconsistent predictions:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42955a73-05fb-4f94-ac04-ffc2465c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with analysis:\n",
    "df_analyzed = analyze_inconsistencies(dataset, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a3280-9cab-4d79-8469-772fff1bf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with styling:\n",
    "styled_df = find_inconsistencies_with_styling(dataset, results)\n",
    "display(styled_df)  # If in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c82749-d5b2-4449-9130-bf93a80b356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics_filepath = save_metrics(metrics, model_name, data_name)\n",
    "\n",
    "# Later, load results and metrics\n",
    "loaded_results = load_analysis_results(results_filepath)\n",
    "loaded_metrics = load_metrics(metrics_filepath)\n",
    "\n",
    "# Verify loaded data\n",
    "print(\"\\nVerifying loaded results...\")\n",
    "print(f\"Number of original results: {len(results)}\")\n",
    "print(f\"Number of loaded results: {len(loaded_results)}\")\n",
    "\n",
    "print(\"\\nVerifying loaded metrics...\")\n",
    "print(f\"Original accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Loaded accuracy: {loaded_metrics['accuracy']:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou1_kernel",
   "language": "python",
   "name": "chou1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
