{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd97d42e-3d03-45d2-9bfd-76752c6712a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efe6ef3-b216-43ad-835a-b02f5efaef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"./pretrained_llms/Llama-3.3-70B-Instruct\"\n",
    "model_path = \"./pretrained_llms/Llama-3.1-8B-Instruct\"\n",
    "data_path = \"./data\"\n",
    "data_name = \"mteb/tweet_sentiment_extraction\"\n",
    "cache_dir = \"./cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb22d9b0-74db-4acb-bf9f-119042fbe5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c12148bae234d818d71e8189751da6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./pretrained_llms/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pad token found, setting pad token to eos token\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          add_eos_token=True,\n",
    "                                          cache_dir=cache_dir)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"No pad token found, setting pad token to eos token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd60285d-d2e2-4ce7-8dca-6e1a43ca8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(data_name, cache_dir=data_path, split='train[10:20]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c37341d-810c-40aa-89cc-88ac84b78b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'LABEL_1', 'score': 0.5922744274139404}\n",
      "{'label': 'LABEL_1', 'score': 0.7167643308639526}\n",
      "{'label': 'LABEL_1', 'score': 0.9328629970550537}\n",
      "{'label': 'LABEL_1', 'score': 0.628308117389679}\n",
      "{'label': 'LABEL_2', 'score': 0.9498377442359924}\n",
      "{'label': 'LABEL_0', 'score': 0.8750986456871033}\n",
      "{'label': 'LABEL_1', 'score': 0.9951352477073669}\n",
      "{'label': 'LABEL_1', 'score': 0.9967533946037292}\n",
      "{'label': 'LABEL_1', 'score': 0.9617916345596313}\n",
      "{'label': 'LABEL_2', 'score': 0.6502686738967896}\n"
     ]
    }
   ],
   "source": [
    "for out in classifier(KeyDataset(dataset, \"text\"), batch_size=8):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e0462c-d13c-4212-a63b-aa3e43751aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[128000, 13617, 1495, 128009, 128009], [128000, 40, 1097, 264, 8334]], 'attention_mask': [[1, 1, 1, 0, 0], [1, 1, 1, 1, 1]]}\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "tokens = classifier.tokenizer([\"Example text\", \"I am a boy\"], padding=True, truncation=True)\n",
    "print(tokens)\n",
    "print(classifier.tokenizer.pad_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou1_kernel",
   "language": "python",
   "name": "chou1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
