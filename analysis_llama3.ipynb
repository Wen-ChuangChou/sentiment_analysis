{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db635cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afe5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Fix Random Seeds\n",
    "seed = 42\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    torch.device(\"cput\")\n",
    "    print(\"NO CUDA is available\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5853918c-8a29-4599-8110-6ebc717317c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = r\"C:\\LLMs\\Meta-Llama-3-8B-Instruct\"\n",
    "# model_path = \"./pretrained_llms/Llama-3.3-70B-Instruct\"\n",
    "model_path = \"./pretrained_llms/Llama-3.1-8B-Instruct\"\n",
    "data_path = \"./data\"\n",
    "data_name = \"mteb/tweet_sentiment_extraction\"\n",
    "cache_dir = \"./cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d587e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720a343413e0426bae79241518fdb363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./pretrained_llms/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(data_name, cache_dir=data_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3, torch_dtype=\"auto\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          add_eos_token=True,\n",
    "                                          cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26f18684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432f5aab298b40d196a308bc0bacf6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd9336225144471bcc1dfd1bf7849eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"No pad token found, setting pad token to eos token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def tokenizer_function(examples):\n",
    "    return tokenizer(examples['text'],\n",
    "                     padding=True,\n",
    "                     truncation=True,\n",
    "                     max_length=512                  \n",
    "                     # return_tensors=\"pt\"\n",
    "                     )\n",
    "\n",
    "\n",
    "# apply tokenizer function on your data\n",
    "tokenized_data = dataset.map(tokenizer_function, batched=False, remove_columns=[\"id\"])\n",
    "train = tokenized_data['train']\n",
    "test = tokenized_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c45f91f6-281e-4538-846f-1833ae5c42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label', 'label_text', 'input_ids', 'attention_mask'])\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train[0].keys())\n",
    "print(type(train[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8c0e08e-1e42-405d-be11-0011416803da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original example:\n",
      "{'id': 'cb774db0d1', 'text': ' I`d have responded, if I were going', 'label': 1, 'label_text': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "# 1. First check what a single example looks like before tokenization\n",
    "print(\"Original example:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ebff98a-6b4a-45d7-80ac-787f75fed725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single tokenized example:\n",
      "{'input_ids': [128000, 358, 63, 67, 617, 16846, 11, 422, 358, 1051, 2133], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 2. Test tokenizer on a single example\n",
    "test_tokenization = tokenizer(\n",
    "    dataset['train'][0]['text'],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "print(\"\\nSingle tokenized example:\")\n",
    "print(test_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df9f6138-84bb-4cb7-b544-04e6596a128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Now try the mapping function\n",
    "def tokenizer_function(example):\n",
    "    result = tokenizer(\n",
    "        example['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    print(\"\\nTokenizer function output:\")\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71509546-fa22-4f7e-83f9-3d3ca24a066e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8373a7bf674a28b3c56068b1c367d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer function output:\n",
      "{'input_ids': [128000, 358, 63, 67, 617, 16846, 11, 422, 358, 1051, 2133], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Tokenizer function output:\n",
      "{'input_ids': [128000, 2100, 2689, 328, 1846, 358, 690, 3194, 499, 1618, 304, 5960, 18842, 12340], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Tokenized dataset structure:\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 4. Process just a few examples\n",
    "test_data = dataset['train'].select(range(2))\n",
    "tokenized_data = test_data.map(tokenizer_function, remove_columns=[\"id\"])\n",
    "print(\"\\nTokenized dataset structure:\")\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5769abe0-9294-4eec-8266-b15fa75830f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples before collation:\n",
      "{'text': [' I`d have responded, if I were going', ' Sooo SAD I will miss you here in San Diego!!!'], 'label': [1, 0], 'label_text': ['neutral', 'negative'], 'input_ids': [[128000, 358, 63, 67, 617, 16846, 11, 422, 358, 1051, 2133], [128000, 2100, 2689, 328, 1846, 358, 690, 3194, 499, 1618, 304, 5960, 18842, 12340]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# 5. Try to see what's in samples before collation\n",
    "samples = tokenized_data[:2]\n",
    "print(\"\\nSamples before collation:\")\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "91edbca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 14, 7, 7, 17, 24, 18, 4]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = train[:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15bd3738-b3e5-4595-8d25-d6053bb09fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[128000, 358, 63, 67, 617, 16846, 11, 422, 358, 1051, 2133],\n",
       " [128000,\n",
       "  2100,\n",
       "  2689,\n",
       "  328,\n",
       "  1846,\n",
       "  358,\n",
       "  690,\n",
       "  3194,\n",
       "  499,\n",
       "  1618,\n",
       "  304,\n",
       "  5960,\n",
       "  18842,\n",
       "  12340],\n",
       " [128000, 2465, 13697, 374, 45647, 757, 1131],\n",
       " [128000, 1148, 7274, 0, 5387, 757, 7636],\n",
       " [128000,\n",
       "  51838,\n",
       "  315,\n",
       "  31804,\n",
       "  11,\n",
       "  3249,\n",
       "  7846,\n",
       "  80215,\n",
       "  814,\n",
       "  2231,\n",
       "  1124,\n",
       "  389,\n",
       "  279,\n",
       "  19786,\n",
       "  584,\n",
       "  2736,\n",
       "  11021],\n",
       " [128000,\n",
       "  1277,\n",
       "  1129,\n",
       "  2185,\n",
       "  16928,\n",
       "  1820,\n",
       "  65,\n",
       "  82401,\n",
       "  916,\n",
       "  2754,\n",
       "  29224,\n",
       "  482,\n",
       "  1063,\n",
       "  21648,\n",
       "  1752,\n",
       "  628,\n",
       "  36368,\n",
       "  369,\n",
       "  279,\n",
       "  1888,\n",
       "  34287,\n",
       "  12111,\n",
       "  389,\n",
       "  9578],\n",
       " [128000,\n",
       "  17,\n",
       "  309,\n",
       "  5510,\n",
       "  826,\n",
       "  369,\n",
       "  279,\n",
       "  8945,\n",
       "  527,\n",
       "  2523,\n",
       "  994,\n",
       "  568,\n",
       "  374,\n",
       "  682,\n",
       "  43632,\n",
       "  323,\n",
       "  1080,\n",
       "  437],\n",
       " [128000, 50, 26845, 1579]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba2a9708",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# {k: v.shape for k, v in batch.items()}\u001b[39;00m\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3388\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3385\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3386\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/p/project1/training2449/chou1/jupyter/kernels/chou1_kernel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = data_collator(samples)\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a4f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128009, 128009]\n",
      "Pad Token id: 128009 and Pad Token: <|eot_id|>\n",
      "EOS Token id: 128009 and EOS Token: <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "print(terminators)\n",
    "print(\n",
    "    f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\"\n",
    ")\n",
    "print(\n",
    "    f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b24ad71-5875-4a11-8e56-a35d052ca9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6105e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer(train[6]['text'], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "# Perform inference\n",
    "\n",
    "example = train[1]\n",
    "input_ids = torch.tensor([example['input_ids']]).to(device)\n",
    "attention_mask = torch.tensor([example['attention_mask']]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Get predicted label\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "predicted_label_text = model.config.id2label[predicted_label]\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc818939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_1\n"
     ]
    }
   ],
   "source": [
    "print(predicted_label_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45bbd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, GPT2Config, GPT2ForSequenceClassification\n",
    "## When training a transformer model,\n",
    "# it’s common to batch sequences together for more efficient processing.\n",
    "# However, since sequences might have different lengths, they need to be padded to a common length within each batch.\n",
    "#The DataCollatorWithPadding class automates this process.\n",
    "\n",
    "#define the collator, use DataCollatorWithPadding() with the defined tokenizer above\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "config = GPT2Config()\n",
    "\n",
    "#define GPT classifier, use 'gpt2' pretrained LLM, we have 3 classes in our dataset\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c39b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# load the accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def acc(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6ee27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training arguments: https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.TrainingArguments\n",
    "# set per_device_train_batch_size  per_device_eval_batch_size as 8,\n",
    "# we will fine tune gpt model for 30 epochs , set the corresponding parameter\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    save_total_limit=2\n",
    "    #gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "# set the data_collator and training arguments we defined above\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=acc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f256c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 06:58, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.626900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.09952616239984831, metrics={'train_runtime': 419.4324, 'train_samples_per_second': 71.525, 'train_steps_per_second': 8.941, 'total_flos': 513256776597504.0, 'train_loss': 0.09952616239984831, 'epoch': 30.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if it outputs memory error: shut down all kernels or restart kernel\n",
    "# train the model, use train() function\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c066d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.218705177307129,\n",
       " 'eval_accuracy': 0.703,\n",
       " 'eval_runtime': 3.8726,\n",
       " 'eval_samples_per_second': 258.225,\n",
       " 'eval_steps_per_second': 32.278,\n",
       " 'epoch': 30.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "#evaluate the fine tuned model on test data; use evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd81382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 2.2688107392809798e-08},\n",
       "  {'label': 'LABEL_1', 'score': 6.458856660174206e-05},\n",
       "  {'label': 'LABEL_2', 'score': 0.9999353885650635}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "#test with new data, use TextClassificationPipeline to prepare your input text\n",
    "\n",
    "# This pipeline has a return_all_scores parameter on its __call__ method that allows you to get all scores for each label on a prediction.\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "# pipe your text here\n",
    "prediction = pipe(\n",
    "    \"I find great pleasure in learning through the NLP courses offered by HLRS.\",\n",
    "    return_all_scores=True)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chou1_kernel",
   "language": "python",
   "name": "chou1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
